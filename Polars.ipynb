{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebaef2c-a0a9-4598-aea1-5bed9608b832",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U polars duckdb pyarrow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "559da9fb",
   "metadata": {},
   "source": [
    "## Polars\n",
    "\n",
    "- Addresses \"10 Things I Hate Abourt Pandas\" - https://wesmckinney.com/blog/apache-arrow-pandas-internals/\n",
    "- Has Series and DataFrame.\n",
    "- Core in Rust\n",
    "- Supports multicore\n",
    "- Supports streaming (larger than RAM)\n",
    "- Lazy (query planning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dde3fb1-2d5f-410e-a64f-c2e9422f2675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335c7a78-a68c-402a-b136-9c9b5d1ee1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aebe807",
   "metadata": {},
   "outputs": [],
   "source": [
    "ser1 = pl.Series(['matt', 'fred', 'suzy'])\n",
    "ser2 = pl.Series([42, 43, 44])\n",
    "df1 = pl.DataFrame({'name': ser1, 'age': ser2,\n",
    "                    'pet': ['cat', 'dog', 'bird']\n",
    "                    })\n",
    "\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36aef8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_pd = df1.to_pandas()\n",
    "df1_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c515541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# common methods\n",
    "print(sorted(set(dir(df1_pd)) & set(dir(df1))))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7b857b9d",
   "metadata": {},
   "source": [
    "## Contexts and Expressions\n",
    "\n",
    "Polars contexts are used to evaluate expressions. \n",
    "\n",
    "### Contexts\n",
    "\n",
    "Types of contexts are available:\n",
    "\n",
    "- Selection Contexts - used to select columns:\n",
    "  - `df.select('column_name')` - only has the selected column\n",
    "  - `df.with_columns(['column_name_1', 'column_name_2'])` - adds the selected columns to the DataFrame\n",
    "- Filter Contexts - used to filter rows:\n",
    "    - `df.filter(pl.col('column_name') > 0)` - filters the DataFrame to only include rows where the given expression is true\n",
    "- Aggregation Contexts - used to aggregate data:\n",
    "  - `df.groupby('column_name').agg(...)` - aggregates the data by the first column and aggregates with the given function (...)\n",
    "- Join Contexts - used to join data:\n",
    "    - `df.join(df2, left_on='column_name', right_on='column_name')` - joins the two DataFrames on the given column names\n",
    "    \n",
    "## DataFrame Methods    \n",
    "\n",
    "### 1. Data Manipulation and Transformation:\n",
    "- **Apply Functions**: `apply`, `map_rows`\n",
    "- **Aggregation**: `max`, `min`, `sum`, `mean`, `median`, `std`, `var`, `product`, `n_unique`, `null_count`, `count`\n",
    "- **Sorting and Ordering**: `sort`, `top_k`, `bottom_k`, `set_sorted`\n",
    "- **Filtering and Selection**: `filter`, `select`, `select_seq`, `drop`, `take_every`\n",
    "- **Missing Value Handling**: `null` and `NaN` are different (float can have both). `fill_nan`, `fill_null`, `drop_nulls`, `interpolate`\n",
    "- **Type Casting**: `cast`, `to_series`, `to_numpy`, `to_pandas`, `to_dict`, `to_dicts`, `to_arrow`, `to_struct`\n",
    "- **Data Reshaping**: `melt`, `pivot`, `transpose`, `unnest`, `unstack`, `explode`, `with_row_count`, `with_columns`, `with_columns_seq`, `insert_at_idx`, `extend`, `to_dummies`\n",
    "- **Combining and Merging**: `join`, `join_asof`, `hstack`, `vstack`, `concat`\n",
    "\n",
    "### 2. Data Inspection and Exploration:\n",
    "- **Basic Information**: `shape`, `width`, `height`, `columns`, `dtypes`, `flags`, `schema`\n",
    "- **Data Retrieval**: `head`, `tail`, `row`, `rows`, `get_column`, `get_columns`, `find_idx_by_name`, `item`\n",
    "- **Data Summary**: `describe`, `glimpse`, `hash_rows`, `corr`\n",
    "- **Data Sampling**: `sample`\n",
    "\n",
    "### 3. Grouping and Aggregation:\n",
    "- `group_by`, `groupby`, `group_by_dynamic`, `groupby_dynamic`, `group_by_rolling`, `groupby_rolling`, `partition_by`\n",
    "- `upsample`\n",
    "\n",
    "### 4. Rolling and Window Functions:\n",
    "- `rolling`\n",
    "\n",
    "### 5. Utility and Miscellaneous:\n",
    "- `clone`, `copy`, `clear`, `shrink_to_fit`, `to_init_repr`, `fold`, `pipe`, `lazy`, `apply`\n",
    "\n",
    "### 6. Data Cleaning and Preprocessing:\n",
    "- `replace`, `update`, `drop_in_place`\n",
    "\n",
    "### 7. Handling Duplicates:\n",
    "- `is_duplicated`, `is_unique`, `unique`\n",
    "\n",
    "### 8. Handling Missing Values:\n",
    "- `drop_nulls`\n",
    "\n",
    "### 9. Data Export and Serialization:\n",
    "- **To File**: `write_csv`, `write_json`, `write_parquet`, `write_excel`, `write_avro`, `write_ipc`, `write_ipc_stream`, `write_ndjson`, `write_database`, `write_delta`\n",
    "- **To Database**: `write_database`\n",
    "\n",
    "### 10. Utility and Miscellaneous:\n",
    "- **Indexing and Slicing**: `slice`, `select_at_idx`, `replace_at_idx`, `shift`, `shift_and_fill`\n",
    "- **Utility**: `reverse`, `n_chunks`, `estimated_size`, `iter_rows`, `iter_slices`, `rows_by_key`, `map_rows`, `merge_sorted`, `rechunk`, `rename`, `is_empty`\n",
    "\n",
    "### 11. Advanced Features:\n",
    "- **Lazy Evaluation**: `lazy`, `collect`\n",
    "\n",
    "### 12. Debugging and Inspection:\n",
    "- `frame_equal`, `glimpse`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de7449d-cdb4-4fec-999e-d1959ac0359e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8042f7-22fc-4cdb-af7a-e116940994dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6c3cf3f5",
   "metadata": {},
   "source": [
    "\n",
    "### Expressions\n",
    "\n",
    "Expressions are used to define the operations to be performed in the context. Polars can optimize the execution of expressions to improve performance. \n",
    "Expressions are found in the `pl` namespace and `pl.col('column_name')`. Some examples include:\n",
    "\n",
    "- `pl.col('column_name')` - references a column\n",
    "- `pl.col(r'.*(_editor)')` - selects all columns that end with '_editor'\n",
    "- `(pl.col('sale') * .3).alias('finders_fee')` - use math to create a new column with the given expression\n",
    "- `(pl.col('age') > 18).alias('is_adult')` - use a boolean expression to create a new column\n",
    "- `pl.all()` - selects all columns\n",
    "- `pl.all().exclude('col1', 'col2')` - selects all columns except the given columns\n",
    "- `pl.col('birth_date').dt.year()` - gets the year from a date column\n",
    "- `pl.col(pl.Float64, pl.Boolean)` - select all float64 and boolean columns\n",
    "- `pl.col(pl.all() - pl.Float64)` - select all columns except float64 columns (set operator)\n",
    "  - `-` (set difference), `&` (set intersection), `|` (set union)\n",
    "- `cs.float()` - select all float columns (using column selectors)\n",
    "- `cs.contains('_editor')` - select all columns that contain '_editor' (using column selectors)\n",
    "- `cs.matches(r'.*(_editor)')` - select all columns that end with '_editor' (using column selectors)\n",
    "- `pl.sum(pl.col('column_name'))` - sum a column\n",
    "- `pl.lit(1)` - creates a literal value\n",
    "\n",
    "If you want to apply operations to the columns that were selected and not the expression, use `.as_expr()`.\n",
    "\n",
    "### Expression Functions\n",
    "\n",
    "### 1. Arithmetic Operations:\n",
    "- **Basic Arithmetic**: `add`, `sub`, `mul`, `div`, `floordiv`, `mod`, `pow`\n",
    "- **Increment/Decrement**: `cumsum`, `cumprod`, `diff`\n",
    "- **Aggregations**: `sum`, `product`, `mean`, `median`, `min`, `max`, `count`, `n_unique`, `nan_min`, `nan_max`, `len`, `any`, `all`, `null_count`\n",
    "\n",
    "### 2. Logical and Comparison Operations:\n",
    "- **Comparison**: `eq`, `ne`, `lt`, `le`, `gt`, `ge`\n",
    "- **Logical**: `and_`, `or_`, `not_`, `xor`\n",
    "- **If/Then/Else**: `pl.when(conditional).then(then_expr).otherwise(else_expr)`\n",
    "\n",
    "### 3. Missing Value Handling:\n",
    "- **Detection**: `is_nan`, `is_not_nan`, `is_null`, `is_not_null`, `is_finite`, `is_infinite`\n",
    "- **Imputation**: `fill_nan`, `fill_null`, `backward_fill`, `forward_fill`, `drop_nans`, `drop_nulls`\n",
    "\n",
    "### 4. String Operations:\n",
    "- `cat`, `str`\n",
    "\n",
    "### 5. List Operations:\n",
    "- **Manipulation**: `arr`, `list`, `flatten`, `explode`, `take`, `append`, `extend_constant`\n",
    "- **Aggregation**: `first`, `last`\n",
    "\n",
    "### 6. Date and Time Operations:\n",
    "- `dt`\n",
    "\n",
    "### 7. Mathematical Functions:\n",
    "- **Trigonometric**: `sin`, `cos`, `tan`, `cot`, `asin`, `acos`, `atan`, `arcsin`, `arccos`, `arctan`, `sinh`, `cosh`, `tanh`, `arcsinh`, `arccosh`, `arctanh`\n",
    "- **Exponential and Logarithmic**: `exp`, `log`, `log10`, `log1p`, `sqrt`, `cbrt`\n",
    "- **Rounding**: `round`, `floor`, `ceil`\n",
    "- **Other**: `abs`, `clip`, `clip_min`, `clip_max`, `degrees`, `radians`\n",
    "\n",
    "### 8. Statistical Functions:\n",
    "- `std`, `var`, `skew`, `kurtosis`, `quantile`, `mode`, `value_counts`\n",
    "\n",
    "### 9. Sorting and Ordering:\n",
    "- `sort`, `sort_by`, `arg_sort`, `arg_min`, `arg_max`, `rank`, `top_k`, `bottom_k`, `search_sorted`, `set_sorted`\n",
    "\n",
    "### 10. Filtering and Searching:\n",
    "- `filter`, `where`, `is_in`, `is_not`, `is_unique`, `is_duplicated`, `is_first`, `is_last`, `is_first_distinct`, `is_last_distinct`, `unique`, `unique_counts`\n",
    "\n",
    "### 11. Window Functions:\n",
    "- `rolling`, `ewm_mean`, `ewm_std`, `ewm_var`, `rolling_apply`, `rolling_max`, `rolling_min`, `rolling_mean`, `rolling_median`, `rolling_std`, `rolling_var`, `rolling_sum`, `rolling_skew`, `rolling_quantile`\n",
    "\n",
    "### 12. Grouping and Aggregation:\n",
    "- `agg_groups`, `groupby`, `over`, `cummax`, `cummin`, `cumcount`, `peak_max`, `peak_min`\n",
    "\n",
    "### 13. Type Casting and Conversion:\n",
    "- `cast`, `to_physical`, `reinterpret`, `shrink_dtype`\n",
    "\n",
    "### 14. JSON and Struct Operations:\n",
    "- `from_json`, `struct`\n",
    "\n",
    "### 15. Sampling and Randomization:\n",
    "- `sample`, `shuffle`\n",
    "\n",
    "### 16. Nameing:\n",
    "- `alias`, `suffix`, `prefix`\n",
    "\n",
    "### 17. Function Application:\n",
    "- `apply`, `pipe`, `map`, `map_batches`, `map_elements`, `map_alias`, `map_dict`\n",
    "\n",
    "### 18. Debugging:\n",
    "- `inspect`, `meta`\n",
    "\n",
    "### 18. Utility and Miscellaneous:\n",
    "- `keep_name`,  `hash`, `repeat_by`, `reshape`, `reverse`, `slice`, `take_every`, `limit`, `head`, `tail`, `cut`, `qcut`, `cumulative_eval`, `pct_change`, `cache`, `rechunk`, `rle`, `rle_id`, `product`, `sign`, `dot`, `exclude`, `implode`, `interpolate`, `shift`, `shift_and_fill`, `select`, `rename`, `lower_bound`, `upper_bound`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048e4742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug selector\n",
    "from polars.selectors import is_selector, expand_selector\n",
    "import polars.selectors as cs\n",
    "\n",
    "eds = cs.matches(r'(name|age)')\n",
    "is_selector(eds), expand_selector(df1, eds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43476c9-c12b-4ce6-bcb3-87f6d67749a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074cb494-e0de-47fa-b53a-976fef9bf61a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f1478941",
   "metadata": {},
   "source": [
    "## Lazy Evaluation\n",
    "\n",
    "Polar's expressions enable lazy evaluation. This means that the expression is not evaluated until it is needed. This can improve performance significantly. Polars supports\n",
    "\n",
    "- Predicate Pushdown - filters are applied as early as possible\n",
    "- Projection Pushdown - only use/read the columns needed\n",
    "\n",
    "Typical flow is to use `pl.scan_csv` to create a lazy DataFrame, or `df.lazy` to create a lazy DataFrame from an existing DataFrame. \n",
    "\n",
    "To materialize the DataFrame, use `df.collect` to execute the plan and return the results. Use `df.collect(streaming=True)` to execute the results in batches. (Not supported by all operations.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf027f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.lazy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e1eed5e0",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e911f15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pyarrow as pa\n",
    "\n",
    "pl.__version__, pa.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4474cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "import urllib.request\n",
    "from zipfile import ZipFile\n",
    "\n",
    "url = 'https://github.com/mattharrison/datasets/raw/master/data/vehicles.csv.zip'\n",
    "\n",
    "# download and unzip the url\n",
    "dest_file = 'vehicles.csv.zip'\n",
    "urllib.request.urlretrieve(url, dest_file)\n",
    "with open(dest_file.replace('.zip', ''), 'wb') as f:\n",
    "    z = ZipFile(dest_file)\n",
    "    f.write(z.read('vehicles.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157dbb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "def make_to_origin(make_col):\n",
    "    # Dictionary mapping car makes to countries of origin\n",
    "    origin_dict = {\n",
    "        'Chevrolet': 'USA',\n",
    "        'Ford': 'USA',\n",
    "        'Dodge': 'USA',\n",
    "        'GMC': 'USA',\n",
    "        'Toyota': 'Japan',\n",
    "        'BMW': 'Germany',\n",
    "        'Mercedes-Benz': 'Germany',\n",
    "        'Nissan': 'Japan',\n",
    "        'Volkswagen': 'Germany',\n",
    "        'Mitsubishi': 'Japan',\n",
    "        'Porsche': 'Germany',\n",
    "        'Mazda': 'Japan',\n",
    "        'Audi': 'Germany',\n",
    "        'Honda': 'Japan',\n",
    "        'Jeep': 'USA',\n",
    "        'Pontiac': 'USA',\n",
    "        'Subaru': 'Japan',\n",
    "        'Volvo': 'Sweden',\n",
    "        'Hyundai': 'South Korea',\n",
    "        'Chrysler': 'USA',\n",
    "        'Tesla': 'USA'\n",
    "    }\n",
    "    \n",
    "    return origin_dict.get(make_col, \"Unknown\")\n",
    "\n",
    "df_pl = pl.read_csv('vehicles-pd.csv')\n",
    "\n",
    "result = (df_pl\n",
    "          .with_columns(pl.col('make').map_elements(make_to_origin).alias(\"origin\"),\n",
    "                pl.col('createdOn').str.to_datetime('%a %b %d %H:%M:%S %Z %Y'))\n",
    "          .filter((pl.col(\"origin\") != \"Unknown\") & (pl.col(\"year\") < 2020))\n",
    "          .select(['make', 'model', 'year', 'city08', 'highway08', 'origin', 'createdOn'])\n",
    "          .group_by(['origin', 'year'])\n",
    "          .agg(pl.col(\"city08\").mean().alias(\"avg_city08\"))\n",
    "          .pivot(index='year', columns='origin', values='avg_city08')\n",
    "          .sort('year')\n",
    ")\n",
    "\n",
    "(result\n",
    "  # leverages Array transfer (use pl.from_pandas to go other way)\n",
    " .to_pandas(use_pyarrow_extension_array=True)\n",
    " .set_index('year')\n",
    " .plot(title='Average Mileage by Year and Country of Origin')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458e89b2-dd4d-4629-9e9a-b5f027108e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "# convert make to origin to expr\n",
    "def make_to_origin_expr(make_col):\n",
    "    # Dictionary mapping car makes to countries of origin\n",
    "    origin_dict = {\n",
    "        'Chevrolet': 'USA',\n",
    "        'Ford': 'USA',\n",
    "        'Dodge': 'USA',\n",
    "        'GMC': 'USA',\n",
    "        'Toyota': 'Japan',\n",
    "        'BMW': 'Germany',\n",
    "        'Mercedes-Benz': 'Germany',\n",
    "        'Nissan': 'Japan',\n",
    "        'Volkswagen': 'Germany',\n",
    "        'Mitsubishi': 'Japan',\n",
    "        'Porsche': 'Germany',\n",
    "        'Mazda': 'Japan',\n",
    "        'Audi': 'Germany',\n",
    "        'Honda': 'Japan',\n",
    "        'Jeep': 'USA',\n",
    "        'Pontiac': 'USA',\n",
    "        'Subaru': 'Japan',\n",
    "        'Volvo': 'Sweden',\n",
    "        'Hyundai': 'South Korea',\n",
    "        'Chrysler': 'USA',\n",
    "        'Tesla': 'USA'\n",
    "    }\n",
    "    expr = None\n",
    "    col = pl.col(make_col)\n",
    "    for k, v in origin_dict.items():\n",
    "        if expr is None:\n",
    "            expr = pl.when(col == k).then(pl.lit(v))\n",
    "            continue\n",
    "        expr = pl.when(col == k).then(pl.lit(v))\n",
    "    expr = expr.otherwise(pl.lit('Unknown'))\n",
    "    return expr\n",
    "\n",
    "df_pl = pl.read_csv('vehicles-pd.csv')\n",
    "\n",
    "result = (df_pl\n",
    "          .with_columns(#pl.col('make').map_elements(make_to_origin).alias(\"origin\"),\n",
    "              make_to_origin_expr('make').alias('origin'),\n",
    "                pl.col('createdOn').str.to_datetime('%a %b %d %H:%M:%S %Z %Y'))\n",
    "          .filter((pl.col(\"origin\") != \"Unknown\") & (pl.col(\"year\") < 2020))\n",
    "          .select(['make', 'model', 'year', 'city08', 'highway08', 'origin', 'createdOn'])\n",
    "          .group_by(['origin', 'year'])\n",
    "          .agg(pl.col(\"city08\").mean().alias(\"avg_city08\"))\n",
    "          .pivot(index='year', columns='origin', values='avg_city08')\n",
    "          .sort('year')\n",
    ")\n",
    "\n",
    "(result\n",
    " .to_pandas(use_pyarrow_extension_array=True)\n",
    " .set_index('year')\n",
    " .plot(title='Average Mileage by Year and Country of Origin')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c36b69-0cee-4ed6-92f6-24b2e8a22a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "(df_pl\n",
    "  .with_columns(#\n",
    "      pl.col('make').map_elements(make_to_origin).alias(\"origin\")\n",
    "  )\n",
    ")\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39d1625-6d16-4530-ab63-16c51b57a041",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "(df_pl\n",
    "  .with_columns(#\n",
    "     make_to_origin_expr('make').alias('origin'),\n",
    "  )\n",
    ")\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0545a59a-9fc5-4c3e-8929-53e44530748a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacfd1fd-90d8-4a28-b315-7cf6e36ae4e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97bdde1-b105-4d3f-ad26-7511e8513e19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "56334fdc",
   "metadata": {},
   "source": [
    "### Exponential Growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33231ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_growth_pl(start, rate, periods):\n",
    "    result = pl.Series([start]*periods)#, dtype='Float64')\n",
    "    for i in range(1, periods):\n",
    "        result[i] = result[i-1] * (1+rate)\n",
    "    return result\n",
    "\n",
    "exponential_growth_pl(1., 0.01, 10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeada7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "exponential_growth_pl(1., 0.01, 10_000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9a3d99-6c82-4c21-9af8-d83ab77bd1ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0687e082-a42c-48c1-9753-bf28a2701b94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d33503-47a3-4ecc-b6a9-99d7c8c8b265",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2473550f",
   "metadata": {},
   "source": [
    "### New Rust UDF\n",
    "https://pola-rs.github.io/polars/user-guide/expressions/plugins/#writing-the-expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e42afe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir rustfn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56ccad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile rustfn/Cargo.toml\n",
    "[package]\n",
    "name = \"expression_lib\"\n",
    "version = \"0.1.0\"\n",
    "edition = \"2021\"\n",
    "\n",
    "[lib]\n",
    "name = \"expression_lib\"\n",
    "crate-type = [\"cdylib\"]\n",
    "\n",
    "[dependencies]\n",
    "polars = { version = \"*\" }\n",
    "pyo3 = { version = \"0.20.0\", features = [\"extension-module\"] }\n",
    "pyo3-polars = { version = \"*\", features = [\"derive\"] }\n",
    "serde = { version = \"1\", features = [\"derive\"] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6756f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write exponential growth as a rust function\n",
    "%%writefile rustfn/exponential_growth.rs\n",
    "use polars::prelude::*;\n",
    "use pyo3_polars::derive::polars_expr;\n",
    "use std::fmt::Write;\n",
    "\n",
    "fn exponential_growth(start: f64, rate: f64, periods: usize) -> PolarsResult<Series> {\n",
    "    let mut result = Series::full(\"growth\", start, periods);\n",
    "    for i in 1..periods {\n",
    "        result.set(i, result.get(i-1) * (1.+rate))?;\n",
    "    }\n",
    "    Ok(result)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1fda46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from polars.type_aliases import IntoExpr\n",
    "from polars.utils.udfs import _get_shared_lib_location\n",
    "\n",
    "lib = _get_shared_lib_location(__file__)\n",
    "\n",
    "@pl.api.register_dataframe_namespace('exponential_growth')\n",
    "class ExponentialGrowth:\n",
    "    def __init__(self, expr: pl.Expr) -> None:\n",
    "        self._expr = expr\n",
    "    \n",
    "    def exponential_growth(self) -> pl.Expr:\n",
    "        return self._expr._register_plugin(\n",
    "            lib=lib,\n",
    "            symbol='exponential_growth',\n",
    "            is_elementwise=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f70d0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install maturin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596cabbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61779732",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25952d7a-2a54-4576-b26d-03fd8c7010a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d39756c-a053-42d9-9bc1-5a206f985adb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd9b163-e666-4ac6-97ad-5360380f36fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d171b0c-db65-4366-8c53-ece42a2fe085",
   "metadata": {},
   "source": [
    "## Laziness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474b5096-2982-4c27-b4c5-5e1d7d05277d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "# convert make to origin to expr\n",
    "def make_to_origin_expr(make_col):\n",
    "    # Dictionary mapping car makes to countries of origin\n",
    "    origin_dict = {\n",
    "        'Chevrolet': 'USA',\n",
    "        'Ford': 'USA',\n",
    "        'Dodge': 'USA',\n",
    "        'GMC': 'USA',\n",
    "        'Toyota': 'Japan',\n",
    "        'BMW': 'Germany',\n",
    "        'Mercedes-Benz': 'Germany',\n",
    "        'Nissan': 'Japan',\n",
    "        'Volkswagen': 'Germany',\n",
    "        'Mitsubishi': 'Japan',\n",
    "        'Porsche': 'Germany',\n",
    "        'Mazda': 'Japan',\n",
    "        'Audi': 'Germany',\n",
    "        'Honda': 'Japan',\n",
    "        'Jeep': 'USA',\n",
    "        'Pontiac': 'USA',\n",
    "        'Subaru': 'Japan',\n",
    "        'Volvo': 'Sweden',\n",
    "        'Hyundai': 'South Korea',\n",
    "        'Chrysler': 'USA',\n",
    "        'Tesla': 'USA'\n",
    "    }\n",
    "    expr = None\n",
    "    col = pl.col(make_col)\n",
    "    for k, v in origin_dict.items():\n",
    "        if expr is None:\n",
    "            expr = pl.when(col == k).then(pl.lit(v))\n",
    "            continue\n",
    "        expr = pl.when(col == k).then(pl.lit(v))\n",
    "    expr = expr.otherwise(pl.lit('Unknown'))\n",
    "    return expr\n",
    "\n",
    "df_pl = pl.scan_csv('vehicles-pd.csv')\n",
    "\n",
    "result = (df_pl\n",
    "          .with_columns(#pl.col('make').map_elements(make_to_origin).alias(\"origin\"),\n",
    "              make_to_origin_expr('make').alias('origin'),\n",
    "                pl.col('createdOn').str.to_datetime('%a %b %d %H:%M:%S %Z %Y'))\n",
    "          .filter((pl.col(\"origin\") != \"Unknown\") & (pl.col(\"year\") < 2020))\n",
    "          .select(['make', 'model', 'year', 'city08', 'highway08', 'origin', 'createdOn'])\n",
    "          .group_by(['origin', 'year'])\n",
    "          .agg(pl.col(\"city08\").mean().alias(\"avg_city08\"))\n",
    "          .pivot(index='year', columns='origin', values='avg_city08')\n",
    "          .sort('year')\n",
    ")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935c0a61-6041-4918-aacf-66608487ed11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = (df_pl\n",
    "          .with_columns(#pl.col('make').map_elements(make_to_origin).alias(\"origin\"),\n",
    "              make_to_origin_expr('make').alias('origin'),\n",
    "                pl.col('createdOn').str.to_datetime('%a %b %d %H:%M:%S %Z %Y'))\n",
    "          .filter((pl.col(\"origin\") != \"Unknown\") & (pl.col(\"year\") < 2020))\n",
    "          .select(['make', 'model', 'year', 'city08', 'highway08', 'origin', 'createdOn'])\n",
    "          .group_by(['origin', 'year'])\n",
    "          .agg(pl.col(\"city08\").mean().alias(\"avg_city08\"))\n",
    "          #.pivot(index='year', columns='origin', values='avg_city08')\n",
    "          #.sort('year')\n",
    ")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7313e83-5135-4e6c-bb4a-6032d3f74b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "df_pl = pl.read_csv('vehicles-pd.csv')\n",
    "\n",
    "result = (df_pl\n",
    "          .with_columns(#pl.col('make').map_elements(make_to_origin).alias(\"origin\"),\n",
    "              make_to_origin_expr('make').alias('origin'),\n",
    "                pl.col('createdOn').str.to_datetime('%a %b %d %H:%M:%S %Z %Y'))\n",
    "          .filter((pl.col(\"origin\") != \"Unknown\") & (pl.col(\"year\") < 2020))\n",
    "          .select(['make', 'model', 'year', 'city08', 'highway08', 'origin', 'createdOn'])\n",
    "          .group_by(['origin', 'year'])\n",
    "          .agg(pl.col(\"city08\").mean().alias(\"avg_city08\"))\n",
    "          #.pivot(index='year', columns='origin', values='avg_city08')\n",
    "          #.sort('year')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c1b20e-d375-4198-b6c9-0302d6f40c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "df_pl = pl.scan_csv('vehicles-pd.csv')\n",
    "\n",
    "result = (df_pl\n",
    "          .with_columns(#pl.col('make').map_elements(make_to_origin).alias(\"origin\"),\n",
    "              make_to_origin_expr('make').alias('origin'),\n",
    "                pl.col('createdOn').str.to_datetime('%a %b %d %H:%M:%S %Z %Y'))\n",
    "          .filter((pl.col(\"origin\") != \"Unknown\") & (pl.col(\"year\") < 2020))\n",
    "          .select(['make', 'model', 'year', 'city08', 'highway08', 'origin', 'createdOn'])\n",
    "          .group_by(['origin', 'year'])\n",
    "          .agg(pl.col(\"city08\").mean().alias(\"avg_city08\"))\n",
    "          #.pivot(index='year', columns='origin', values='avg_city08')\n",
    "          #.sort('year')\n",
    "          .collect()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4cd901-7fd9-4cd2-91b1-1fe2828f34d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare to pandas\n",
    "import pandas as pd\n",
    "def make_to_origin(make):\n",
    "    \"\"\"\n",
    "    Convert car make to country of origin.\n",
    "    \n",
    "    Args:\n",
    "        make (str): Car make.\n",
    "        \n",
    "    Returns:\n",
    "        str: Country of origin.\n",
    "    \"\"\"\n",
    "    # Dictionary mapping car makes to countries of origin\n",
    "    origin_dict = {\n",
    "        'Chevrolet': 'USA',\n",
    "        'Ford': 'USA',\n",
    "        'Dodge': 'USA',\n",
    "        'GMC': 'USA',\n",
    "        'Toyota': 'Japan',\n",
    "        'BMW': 'Germany',\n",
    "        'Mercedes-Benz': 'Germany',\n",
    "        'Nissan': 'Japan',\n",
    "        'Volkswagen': 'Germany',\n",
    "        'Mitsubishi': 'Japan',\n",
    "        'Porsche': 'Germany',\n",
    "        'Mazda': 'Japan',\n",
    "        'Audi': 'Germany',\n",
    "        'Honda': 'Japan',\n",
    "        'Jeep': 'USA',\n",
    "        'Pontiac': 'USA',\n",
    "        'Subaru': 'Japan',\n",
    "        'Volvo': 'Sweden',\n",
    "        'Hyundai': 'South Korea',\n",
    "        'Chrysler': 'USA',\n",
    "        'Tesla': 'USA'\n",
    "    }\n",
    "    \n",
    "    return origin_dict.get(make, \"Unknown\")\n",
    "url = 'https://github.com/mattharrison/datasets/raw/master/data/vehicles.csv.zip'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c3fd48-2c17-4731-8ef8-4baa7e3b4afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "df_pd = pd.read_csv(url,\n",
    "                 engine='pyarrow', dtype_backend='pyarrow')\n",
    "(df_pd\n",
    " .assign(origin=lambda df: df['make'].apply(make_to_origin),\n",
    "         # replace EST and EDT with offset in createdOn\n",
    "        createdOn=lambda df: df['createdOn'].str.replace('EDT', '-04:00').str.replace('EST', '-05:00')\n",
    " )\n",
    " .assign(\n",
    "        # convert createdOn to datetime using strftime for  Tue Jan 01 00:00:00 -05:00 2013\n",
    "        # has mixed timezones so we need to use utc=True\n",
    "        createdOn=lambda df: pd.to_datetime(df['createdOn'], format='%a %b %d %H:%M:%S %z %Y', utc=True),\n",
    "        )\n",
    " .query('origin != \"Unknown\" and year < 2020')\n",
    " .loc[:, ['make', 'model', 'year', 'city08', 'highway08', 'origin', 'createdOn']]\n",
    "    .groupby(['origin', 'year'])\n",
    "    .city08\n",
    "    .mean()\n",
    "  #  .unstack('origin')\n",
    "  #.plot(title='Average Mileage by Year and Country of Origin')\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a786515-c57b-4c80-b0b4-17044832d571",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59839a91-94d5-44fd-b059-53c0cd38a0b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cddf690-1306-46e3-beba-6f889e15af85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c5e223-7729-4104-8e70-293b26b14171",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd5ef92-7c8c-4f23-844e-b3847e6f5cf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "511fd5d5",
   "metadata": {},
   "source": [
    "## Polars Exercises\n",
    "\n",
    "1. **Basic DataFrame Operations**\n",
    "\n",
    "   - Show the shape of the `df_pl` DataFrame.\n",
    "   - Print the first 5 rows of the `df_pl` DataFrame.\n",
    "   - Print the last 5 rows of the `df_pl` DataFrame.\n",
    "   - Print the list of columns in the `df_pl` DataFrame.\n",
    "   \n",
    "2. **Data Exploration**\n",
    "   - Print the number of unique values in each column of the `df_pl` DataFrame.\n",
    "   - Print the number of null values in each column of the `df_pl` DataFrame. (Hint: see `df.select` and `pl.all`)\n",
    "   - Print the mean and standard deviation of the 'city08' column of the `df_pl` DataFrame.\n",
    "   - Print the median and 75th percentile of the 'city08' column of the `df_pl` DataFrame.\n",
    "\n",
    "3. **String Manipulation**\n",
    "   - Upper case the 'make' column of the `df_pl` DataFrame. (Hint: see `dir(pl.col('make').str)` )\n",
    "   - Combine the 'year' and 'make' columns of the `df_pl` DataFrame into a new column called 'year_make'. (Hint: see `pl.Col.cast`)\n",
    "\n",
    "4. **Datetime Conversion**\n",
    "   - Convert the 'createdOn' column to the New York timezone. (Hint: see `pl.Col.dt.replace_timezone`)\n",
    "\n",
    "5. **Data Filtering**\n",
    "   - Filter the `df_pl` DataFrame to only include rows where the 'make' column is 'Ford'. (Hint: see `df.filter`)\n",
    "   - Filter the data to only include rows where the 'model' column is a single word. (Hint: see `pl.Col.str.split` ane `pl.Col.list.lengths`)\n",
    "   - Filter the rows where the city mileage is greater than 75% of the city mileage values.\n",
    "\n",
    "6. **Grouping and Aggregation (Moderate)**\n",
    "   - Find the average mileage for Ford, Tesla, and Toyota vehicles.\n",
    "   - Find the average mileage by year and make\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "391a112c5d8063bb1ed75e17f94ea9abfe7bf72365421a70d5e3491b1de003be"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
