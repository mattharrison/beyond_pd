{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebaef2c-a0a9-4598-aea1-5bed9608b832",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -U pandas polars duckdb pyarrow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c07808e5",
   "metadata": {},
   "source": [
    "# Pandas 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1f61f1e2",
   "metadata": {},
   "source": [
    "New features of Pandas 2 includes:\n",
    "\n",
    "- Pyarrow integration\n",
    "  - Speed up IO and calculations\n",
    "  - Support for missing values\n",
    "- Copy-on-write\n",
    "\n",
    "Overlooked features of Pandas API:\n",
    "\n",
    "- Ponder (Snowflake) scales out Pandas code\n",
    "- CuDF scales out to GPUs\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e29f560e",
   "metadata": {},
   "source": [
    "## DataFrame Methods in Pandas 2\n",
    "\n",
    "### 1. Data Manipulation and Transformation:\n",
    "- **Arithmetic Operations**: `add`, `sub`, `mul`, `div`, `floordiv`, `mod`, `pow`, `dot`, `cummax`, `cummin`, `cumprod`, `cumsum`\n",
    "- **Data Transformation**: `apply`, `applymap`, `transform`, `map`, `agg`, `aggregate`\n",
    "- **Type Conversion**: `astype`, `convert_dtypes`, `infer_objects`\n",
    "- **Data Reshaping**: `melt`, `pivot`, `pivot_table`, `stack`, `unstack`, `explode`, `get_dummies`\n",
    "- **String Manipulation**: `add_prefix`, `add_suffix`\n",
    "- **Data Combining**: `combine`, `combine_first`, `merge`, `join`\n",
    "- **Data Cleaning**: `replace`, `drop`, `drop_duplicates`, `filter`, `clip`, `mask`, `where`, `truncate`\n",
    "- **Data Sampling and Randomization**: `sample`\n",
    "- **Data Sorting and Ordering**: `sort_values`, `sort_index`, `nsmallest`, `nlargest`\n",
    "\n",
    "### 2. Data Retrieval and Indexing:\n",
    "- **Selection and Indexing**: `loc`, `iloc`, `iat`, `at`, `xs`, `get`, `item`, `pop`, `query`\n",
    "- **Data Retrieval**: `head`, `tail`, \n",
    "\n",
    "### 3. Aggregation and Descriptive Statistics:\n",
    "- `sum`, `mean`, `median`, `min`, `max`, `mode`, `std`, `var`, `skew`, `kurt`, `kurtosis`, `quantile`, `count`, `corr`, `cov`, `rank`, `pct_change`, `sem`, `all`, `any`, `first`, `last`\n",
    "\n",
    "### 4. Grouping and Aggregation:\n",
    "- `groupby`, `resample`, `expanding`, `ewm`, `rolling`\n",
    "\n",
    "### 5. Handling Missing Values:\n",
    "- `isna`, `isnull`, `notna`, `notnull`, `dropna`, `fillna`, `interpolate`\n",
    "\n",
    "### 6. Data Inspection and Information:\n",
    "- `info`, `describe`, `shape`, `size`, `ndim`, `empty`, `memory_usage`, `dtype`, `dtypes`, `axes`\n",
    "\n",
    "### 7. File I/O and Serialization:\n",
    "- **To File**: `to_csv`, `to_excel`, `to_json`, `to_html`, `to_latex`, `to_markdown`, `to_string`, `to_clipboard`, `to_feather`, `to_parquet`, `to_stata`, `to_gbq`, `to_orc`, `to_pickle`, `to_sql`, `to_records`, `to_dict`, `to_xarray`, `to_xml`\n",
    "- **From File/Records**: `from_dict`, `from_records`\n",
    "\n",
    "### 8. Time Series and Date Handling:\n",
    "- `asfreq`, `asof`, `to_period`, `to_timestamp`, `tz_convert`, `tz_localize`, `at_time`, `between_time`, `resample`\n",
    "\n",
    "### 9. Data Alignment and Missing Value Handling:\n",
    "- `align`, `backfill`, `bfill`, `ffill`, `pad`, `reindex`, `reindex_like`\n",
    "\n",
    "### 10. Data Styling and Visualization:\n",
    "- `style`, `plot`, `boxplot`, `hist`\n",
    "\n",
    "### 11. Utility and Miscellaneous:\n",
    "- `copy`, `equals`, `isin`, `duplicated`,  `idxmax`, `idxmin`, `first_valid_index`, `last_valid_index`, `keys`, `items`, `iterrows`, `itertuples`, `set_axis`, `set_flags`, `set_index`, `reset_index`, `reorder_levels`, `swapaxes`, `swaplevel`, `update`, `pipe`, `squeeze`, `equals`, `compare`, `value_counts`\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5cc83aac",
   "metadata": {},
   "source": [
    "## Pandas Functions\n",
    "Features in the `pd` namespace:\n",
    "\n",
    "### 1. Data Manipulation and Transformation:\n",
    "- **Data Reshaping**: `lreshape`, `wide_to_long`, `crosstab`\n",
    "- **Combining and Merging**: `concat`, \n",
    "- **Conversion and Casting**: `to_datetime`, `to_timedelta`, `to_numeric`\n",
    "- **Factorization and Binning**: `factorize`, `cut`, `qcut`\n",
    "- **Dummy Variable Encoding**: `get_dummies`, `from_dummies`\n",
    "\n",
    "### 2. Data Loading and IO Operations:\n",
    "- **Reading Data**: `read_csv`, `read_excel`, `read_json`, `read_html`, `read_sql`, `read_sql_query`, `read_sql_table`, `read_parquet`, `read_pickle`, `read_hdf`, `read_feather`, `read_stata`, `read_sas`, `read_spss`, `read_gbq`, `read_orc`, `read_fwf`, `read_clipboard`, `read_table`, `read_xml`\n",
    "- **Saving Data**: `to_pickle`\n",
    "\n",
    "### 3. Options and Configuration:\n",
    "- `set_option`, `get_option`, `reset_option`, `describe_option`, `option_context`, `set_eng_float_format`\n",
    "\n",
    "### 4. Data Inspection and Information:\n",
    "- `show_versions`, `testing`, `test`\n",
    "\n",
    "\n",
    "### 5. Time Series and Date Handling:\n",
    "- `date_range`, `bdate_range`, `period_range`, `timedelta_range`, `infer_freq`, `offsets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de07f8b-b732-4258-8aad-6a0355abfdab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cfe702-25a4-43a5-abfc-588c9e329a1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f39f070c-4b1d-42ae-8d0a-7c04ec835b45",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba358f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "url = 'https://github.com/mattharrison/datasets/raw/master/data/vehicles.csv.zip'\n",
    "df_pd = pd.read_csv(url,\n",
    "                 engine='pyarrow', dtype_backend='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c86a75a-cc54-47b0-87e7-27b3d48a1195",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6575cd31-31a2-4df4-8884-f90a863a1141",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83e415a4-4e95-40b1-a4a5-2af0bc36f962",
   "metadata": {},
   "source": [
    "## Exercise API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6112ac7c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 38\u001b[0m\n\u001b[1;32m     12\u001b[0m     origin_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mChevrolet\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUSA\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFord\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUSA\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTesla\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUSA\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     34\u001b[0m     }\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m origin_dict\u001b[38;5;241m.\u001b[39mget(make, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 38\u001b[0m (\u001b[43mdf_pd\u001b[49m\n\u001b[1;32m     39\u001b[0m  \u001b[38;5;241m.\u001b[39massign(origin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m df: df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmake\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(make_to_origin),\n\u001b[1;32m     40\u001b[0m          \u001b[38;5;66;03m# replace EST and EDT with offset in createdOn\u001b[39;00m\n\u001b[1;32m     41\u001b[0m         createdOn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m df: df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcreatedOn\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEDT\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-04:00\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEST\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-05:00\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     42\u001b[0m  )\n\u001b[1;32m     43\u001b[0m  \u001b[38;5;241m.\u001b[39massign(\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;66;03m# convert createdOn to datetime using strftime for  Tue Jan 01 00:00:00 -05:00 2013\u001b[39;00m\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;66;03m# has mixed timezones so we need to use utc=True\u001b[39;00m\n\u001b[1;32m     46\u001b[0m         createdOn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m df: pd\u001b[38;5;241m.\u001b[39mto_datetime(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcreatedOn\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%a\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mb \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mz \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m'\u001b[39m, utc\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     47\u001b[0m         )\n\u001b[1;32m     48\u001b[0m  \u001b[38;5;241m.\u001b[39mquery(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morigin != \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and year < 2020\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     49\u001b[0m  \u001b[38;5;241m.\u001b[39mloc[:, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmake\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcity08\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhighway08\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124morigin\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcreatedOn\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morigin\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;241m.\u001b[39mcity08\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;241m.\u001b[39munstack(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morigin\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     54\u001b[0m   \u001b[38;5;241m.\u001b[39mplot(title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAverage Mileage by Year and Country of Origin\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     55\u001b[0m  )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_pd' is not defined"
     ]
    }
   ],
   "source": [
    "def make_to_origin(make):\n",
    "    \"\"\"\n",
    "    Convert car make to country of origin.\n",
    "    \n",
    "    Args:\n",
    "        make (str): Car make.\n",
    "        \n",
    "    Returns:\n",
    "        str: Country of origin.\n",
    "    \"\"\"\n",
    "    # Dictionary mapping car makes to countries of origin\n",
    "    origin_dict = {\n",
    "        'Chevrolet': 'USA',\n",
    "        'Ford': 'USA',\n",
    "        'Dodge': 'USA',\n",
    "        'GMC': 'USA',\n",
    "        'Toyota': 'Japan',\n",
    "        'BMW': 'Germany',\n",
    "        'Mercedes-Benz': 'Germany',\n",
    "        'Nissan': 'Japan',\n",
    "        'Volkswagen': 'Germany',\n",
    "        'Mitsubishi': 'Japan',\n",
    "        'Porsche': 'Germany',\n",
    "        'Mazda': 'Japan',\n",
    "        'Audi': 'Germany',\n",
    "        'Honda': 'Japan',\n",
    "        'Jeep': 'USA',\n",
    "        'Pontiac': 'USA',\n",
    "        'Subaru': 'Japan',\n",
    "        'Volvo': 'Sweden',\n",
    "        'Hyundai': 'South Korea',\n",
    "        'Chrysler': 'USA',\n",
    "        'Tesla': 'USA'\n",
    "    }\n",
    "    \n",
    "    return origin_dict.get(make, \"Unknown\")\n",
    "\n",
    "(df_pd\n",
    " .assign(origin=lambda df: df['make'].apply(make_to_origin),\n",
    "         # replace EST and EDT with offset in createdOn\n",
    "        createdOn=lambda df: df['createdOn'].str.replace('EDT', '-04:00').str.replace('EST', '-05:00')\n",
    " )\n",
    " .assign(\n",
    "        # convert createdOn to datetime using strftime for  Tue Jan 01 00:00:00 -05:00 2013\n",
    "        # has mixed timezones so we need to use utc=True\n",
    "        createdOn=lambda df: pd.to_datetime(df['createdOn'], format='%a %b %d %H:%M:%S %z %Y', utc=True),\n",
    "        )\n",
    " .query('origin != \"Unknown\" and year < 2020')\n",
    " .loc[:, ['make', 'model', 'year', 'city08', 'highway08', 'origin', 'createdOn']]\n",
    "    .groupby(['origin', 'year'])\n",
    "    .city08\n",
    "    .mean()\n",
    "    .unstack('origin')\n",
    "  .plot(title='Average Mileage by Year and Country of Origin')\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c416783",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd.to_csv('vehicles-pd.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3234c760-3cf4-4d99-9224-db333ed374b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8627e5-95d0-4500-a2e2-4a835992158b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "63582b6c",
   "metadata": {},
   "source": [
    "## Exponential Growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d1f982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I invest $1 and it grows by 1% every day for 1,000 days\n",
    "# how would I calculate the value with pandas?\n",
    "\n",
    "investment = pd.Series([1])\n",
    "\n",
    "def compound_growth(start, rate, periods):\n",
    "    result = pd.Series([start]*periods, dtype='float64[pyarrow]')\n",
    "    for i in range(1, periods):\n",
    "        result[i] = result[i-1] * (1+rate)\n",
    "    return result\n",
    "\n",
    "compound_growth(1, 0.01, 10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2789ad93",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "compound_growth(1, 0.01, 10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f16b25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy version\n",
    "def compound_growth_np(start, rate, periods):\n",
    "    result = np.empty(periods, dtype='float64')\n",
    "    result[0] = start\n",
    "    for i in range(1, periods):\n",
    "        result[i] = result[i-1] * (1+rate)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7313c058",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "compound_growth_np(1, 0.01, 10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33520e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert compound_growth to a numba function with types\n",
    "import numba\n",
    "import numpy as np\n",
    "\n",
    "@numba.njit('float64[:](float64, float64, int64)')\n",
    "def compound_growth_nb(start, rate, periods):\n",
    "    result = np.empty(periods, dtype='float64')\n",
    "    result[0] = start\n",
    "    for i in range(1, periods):\n",
    "        result[i] = result[i-1] * (1+rate)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8409a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "compound_growth_nb(1, 0.01, 10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8892c0-5579-429c-9a44-002eb526d9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cython\n",
    "cython.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af77ff96",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097a731a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "import numpy as np\n",
    "cimport numpy as cnp \n",
    "cnp.import_array()\n",
    "\n",
    "DTYPE = np.float64\n",
    "ctypedef cnp.float64_t DTYPE_t\n",
    "\n",
    "cimport cython\n",
    "\n",
    "def compound_growth_cy(float start, float rate, int periods):\n",
    "    # https://cython.readthedocs.io/en/latest/src/tutorial/numpy.html#:~:text=Efficient%20indexing-,%C2%B6,-There%E2%80%99s%20still%20a\n",
    "    cdef cnp.ndarray[DTYPE_t, ndim=1] result = np.zeros(periods, dtype=DTYPE)\n",
    "    #cdef cnp.ndarray result = np.zeros(periods, dtype=DTYPE)\n",
    "    result[0] = start\n",
    "    cdef int i\n",
    "    cdef DTYPE_t value \n",
    "    for i in range(1, periods):\n",
    "        value = result[i-1] * (1+rate)\n",
    "        result[i] = value\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90af17a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "compound_growth_cy(1, 0.01, 10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72146524",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(compound_growth_cy(1, 0.01, 10_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ec9999",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.Series(compound_growth_nb(1, 0.01, 10_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d48b0d8-92ea-4ee2-9cf7-bb1b32e6b658",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63092f7e-0d64-4f08-b7da-f796f01600ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab2495c-3efa-431c-9d7e-93438c50042b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9b22610-b28e-4c0e-b064-a7250f1e356b",
   "metadata": {},
   "source": [
    "## Copy on Write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5ac5c9-135a-4674-bd86-e376a1ba143c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.copy_on_write = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e180df4-474e-4d37-b627-cfb8b456b0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5578ea-f204-46b2-ab64-b0153b89aba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "\n",
    "import os\n",
    "\n",
    "class MemoryTracker:\n",
    "    def __init__(self):\n",
    "        self.previous_memory = self._get_process_memory()\n",
    "\n",
    "    def _get_process_memory(self):\n",
    "        process = psutil.Process(os.getpid())\n",
    "        memory_info = process.memory_info()\n",
    "        return memory_info.rss / (1024 ** 2)  # Convert bytes to megabytes\n",
    "\n",
    "    def __call__(self, df, txt=''):\n",
    "        current_memory = self._get_process_memory()\n",
    "        memory_growth = current_memory - self.previous_memory\n",
    "        print(f\"{txt} Process memory usage: {current_memory:.2f} MB (growth: {memory_growth:.2f} MB)\")\n",
    "        self.previous_memory = current_memory\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8191f15d-f513-412b-865e-65e66ec6bb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_to_origin(make):\n",
    "    \"\"\"\n",
    "    Convert car make to country of origin.\n",
    "    \n",
    "    Args:\n",
    "        make (str): Car make.\n",
    "        \n",
    "    Returns:\n",
    "        str: Country of origin.\n",
    "    \"\"\"\n",
    "    # Dictionary mapping car makes to countries of origin\n",
    "    origin_dict = {\n",
    "        'Chevrolet': 'USA',\n",
    "        'Ford': 'USA',\n",
    "        'Dodge': 'USA',\n",
    "        'GMC': 'USA',\n",
    "        'Toyota': 'Japan',\n",
    "        'BMW': 'Germany',\n",
    "        'Mercedes-Benz': 'Germany',\n",
    "        'Nissan': 'Japan',\n",
    "        'Volkswagen': 'Germany',\n",
    "        'Mitsubishi': 'Japan',\n",
    "        'Porsche': 'Germany',\n",
    "        'Mazda': 'Japan',\n",
    "        'Audi': 'Germany',\n",
    "        'Honda': 'Japan',\n",
    "        'Jeep': 'USA',\n",
    "        'Pontiac': 'USA',\n",
    "        'Subaru': 'Japan',\n",
    "        'Volvo': 'Sweden',\n",
    "        'Hyundai': 'South Korea',\n",
    "        'Chrysler': 'USA',\n",
    "        'Tesla': 'USA'\n",
    "    }\n",
    "    \n",
    "    return origin_dict.get(make, \"Unknown\")\n",
    "\n",
    "mt = MemoryTracker()\n",
    "\n",
    "\n",
    "(df_pd\n",
    " .pipe(mt, txt='Start')\n",
    " .assign(origin=lambda df: df['make'].apply(make_to_origin),\n",
    "         # replace EST and EDT with offset in createdOn\n",
    "        createdOn=lambda df: df['createdOn'].str.replace('EDT', '-04:00').str.replace('EST', '-05:00')\n",
    " )\n",
    " .pipe(mt, txt='assign')\n",
    " .assign(\n",
    "        # convert createdOn to datetime using strftime for  Tue Jan 01 00:00:00 -05:00 2013\n",
    "        # has mixed timezones so we need to use utc=True\n",
    "        createdOn=lambda df: pd.to_datetime(df['createdOn'], format='%a %b %d %H:%M:%S %z %Y', utc=True),\n",
    "        )\n",
    " .pipe(mt, txt='assign2')\n",
    " .query('origin != \"Unknown\" and year < 2020')\n",
    " .pipe(mt, txt='query')\n",
    " .loc[:, ['make', 'model', 'year', 'city08', 'highway08', 'origin', 'createdOn']]\n",
    " .pipe(mt, txt='loc')\n",
    " .groupby(['origin', 'year'])\n",
    " .city08\n",
    " .mean()\n",
    " .pipe(mt, txt='grouping')\n",
    " .unstack('origin')\n",
    " .pipe(mt, txt='unstack')\n",
    "  #.plot(title='Average Mileage by Year and Country of Origin')\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34433fd-607a-444e-a2a2-e77002db4810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_to_origin(make):\n",
    "    \"\"\"\n",
    "    Convert car make to country of origin.\n",
    "    \n",
    "    Args:\n",
    "        make (str): Car make.\n",
    "        \n",
    "    Returns:\n",
    "        str: Country of origin.\n",
    "    \"\"\"\n",
    "    # Dictionary mapping car makes to countries of origin\n",
    "    origin_dict = {\n",
    "        'Chevrolet': 'USA',\n",
    "        'Ford': 'USA',\n",
    "        'Dodge': 'USA',\n",
    "        'GMC': 'USA',\n",
    "        'Toyota': 'Japan',\n",
    "        'BMW': 'Germany',\n",
    "        'Mercedes-Benz': 'Germany',\n",
    "        'Nissan': 'Japan',\n",
    "        'Volkswagen': 'Germany',\n",
    "        'Mitsubishi': 'Japan',\n",
    "        'Porsche': 'Germany',\n",
    "        'Mazda': 'Japan',\n",
    "        'Audi': 'Germany',\n",
    "        'Honda': 'Japan',\n",
    "        'Jeep': 'USA',\n",
    "        'Pontiac': 'USA',\n",
    "        'Subaru': 'Japan',\n",
    "        'Volvo': 'Sweden',\n",
    "        'Hyundai': 'South Korea',\n",
    "        'Chrysler': 'USA',\n",
    "        'Tesla': 'USA'\n",
    "    }\n",
    "    \n",
    "    return origin_dict.get(make, \"Unknown\")\n",
    "\n",
    "pd.options.mode.copy_on_write = False\n",
    "mt = MemoryTracker()\n",
    "\n",
    "\n",
    "(df_pd\n",
    " .pipe(mt, txt='Start')\n",
    " .assign(origin=lambda df: df['make'].apply(make_to_origin),\n",
    "         # replace EST and EDT with offset in createdOn\n",
    "        createdOn=lambda df: df['createdOn'].str.replace('EDT', '-04:00').str.replace('EST', '-05:00')\n",
    " )\n",
    " .pipe(mt, txt='assign')\n",
    " .assign(\n",
    "        # convert createdOn to datetime using strftime for  Tue Jan 01 00:00:00 -05:00 2013\n",
    "        # has mixed timezones so we need to use utc=True\n",
    "        createdOn=lambda df: pd.to_datetime(df['createdOn'], format='%a %b %d %H:%M:%S %z %Y', utc=True),\n",
    "        )\n",
    " .pipe(mt, txt='assign2')\n",
    " .query('origin != \"Unknown\" and year < 2020')\n",
    " .pipe(mt, txt='query')\n",
    " .loc[:, ['make', 'model', 'year', 'city08', 'highway08', 'origin', 'createdOn']]\n",
    " .pipe(mt, txt='loc')\n",
    " .groupby(['origin', 'year'])\n",
    " .city08\n",
    " .mean()\n",
    " .pipe(mt, txt='grouping')\n",
    " .unstack('origin')\n",
    " .pipe(mt, txt='unstack')\n",
    "  #.plot(title='Average Mileage by Year and Country of Origin')\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8f251e-3873-4ab1-b107-5a6a734212b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e3f5df-a7fb-40e2-a431-3cd3c86a9a3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33300059-7ba3-4c71-bc78-894533b99639",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f498c30",
   "metadata": {},
   "source": [
    "# Pandas Exercises\n",
    "\n",
    "1. **Basic DataFrame Operations**\n",
    "   - Show the shape of the `df_pd` DataFrame.\n",
    "   - Print the first 5 rows of the `df_pd` DataFrame.\n",
    "   - Print the last 5 rows of the `df_pd` DataFrame.\n",
    "   - Print the list of columns in the `df_pd` DataFrame.\n",
    "\n",
    "2. **Data Exploration**\n",
    "   - Print the number of unique values in each column of the `df_pd` DataFrame.\n",
    "   - Print the number of null values in each column of the `df_pd` DataFrame.\n",
    "   - Print the mean and standard deviation of the 'city08' column of the `df_pd` DataFrame.\n",
    "   - Print the median and 75th percentile of the 'city08' column of the `df_pd` DataFrame.\n",
    "\n",
    "4. **String Manipulation**\n",
    "   - Upper case the 'make' column of the `df_pd` DataFrame.\n",
    "   - Combine the 'year' and 'make' columns of the `df_pd` DataFrame into a new column called 'year_make'.\n",
    "\n",
    "5. **Datetime Conversion**\n",
    "   - Convert the 'createdOn' column to the New York timezone.\n",
    "\n",
    "6. **Data Filtering**\n",
    "   - Filter the `df_pd` DataFrame to only include rows where the 'make' column is 'Ford'.\n",
    "   - Filter the data to only include rows where the 'model' column is a single word.\n",
    "   - Filter the rows where the city mileage is greater than 75% of the city mileage values.\n",
    "\n",
    "7. **Grouping and Aggregation (Moderate)**\n",
    "   - Find the average mileage for Ford, Tesla, and Toyota vehicles.\n",
    "   - Find the average mileage by year and make\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4640d787-40df-4b75-8405-b83b86623153",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5975c8-ffc4-4152-8ed0-3cbc6e09cab0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510c389d-7e97-4ad7-94bc-f6171bce1ca9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa57861-21e5-4cb7-a14d-e99fdca90394",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3abcef-54ad-413a-995e-1239daddf373",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "391a112c5d8063bb1ed75e17f94ea9abfe7bf72365421a70d5e3491b1de003be"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
